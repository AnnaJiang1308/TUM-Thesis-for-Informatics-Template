

\chapter{Background}\label{chapter:background}


\section{Large Language Model}
Large Language Models (LLMs) are highly complex artificial intelligence systems that can learn from the vast amounts of available text data\cite{radford2018improving}. Thanks to the attending of \textit{Transformer} \cite{vaswani2017attention}, a deep learning architecture, these language models which employed self-supervised pre-training have demonstrated improved efficiency and scalability in many fields.
 Based on self-attention mechanisms and feed-forward module,\textit{Transformer} has overwhelming advantages in computing representations and global dependencies.

In concrete terms, the Large Language Models equipped with \textit{Transformer} are capable of diverse tasks raised by Natural Language Processing\cite{chowdhary2020natural}, such as textual entailment, question answering, semantic similarity assessment, and document classification. Take BERT\cite{alaparthi2020bidirectional} and GPT \cite{radford2018improving,radford2019language,brown2020language} as two examples, the former utilizes transformer encoder blocks to predict missing words in a given text, and the latter has been enjoying a tremendous reputation for generating diverse and human-like responses, showcasing its potential in various domains.

    \subsection{Text Classification}
In our project, a bunch of hierarchical activity labels would be acquired from the dataset \textit{drive \& act} to depict every detail of the participant's movements in the driving behavior recorded in the video. These labels, however, are too trivial for the construction of learning data, as considering each activity individually will be tedious in such a vast and complex model training process. Therefore, labels should be classified according to the object on which this behavior operates or the specificity of the moment in which the action takes place. For example, the fastening of a seat belt should occur shortly after entering the vehicle, and all behavior related to eating or drinking should be classified into the same group.

In our task, we use zero-shot text classification. This is a task where a model is trained on a set of labeled examples and then classifies new examples from previously unseen classes. This method, which leverages a pre-trained language model, can be thought of as an instance of transfer learning which generally refers to using a model trained for one task in a different application than what it was originally trained for. This is particularly useful for situations where the amount of labeled data is small, for example, our work with 39 different behaviors to be classified.

The model used in the pipeline is \textit{BART} \cite{lewis2019bartdenoisingsequencetosequencepretraining}. According to the paper, BART is trained by corrupting text with an arbitrary noising function and learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. With all these prerequisites it is quite obverious that \textit{BART} is a very practical model for our classification task.

\section{graph}
In graph theory, a graph is a structure made up of a collection of objects, where certain pairs of these objects are connected in a specific way\cite{zhou2020graph}.As a powerful tool for modeling and analyzing complex systems, researchers have been combined graph theory with deep learning to solve various problems in different fields, such as social networks\cite{wu2020graph}, complex physic system\cite{sanchez2018graph} and Protein-protein interactions (PPIs)\cite{NIPS2017_f5077839}.

A Graph is $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ is a pair of sets, where nodes $\mathcal{V}$





    \subsection{Graph Neural Networks}

    \subsection{Dynamic graph}
Dynamic graphs are a type of graph, the input feature or topology of which changes over time.


    

\section{behavior prediction}



