## 01_Introduction
@article{inagaki2019critique,
  title={A critique of the SAE conditional driving automation definition, and analyses of options for improvement},
  author={Inagaki, Toshiyuki and Sheridan, Thomas B},
  journal={Cognition, technology \& work},
  volume={21},
  pages={569--578},
  year={2019},
  publisher={Springer}
}

@article{lee2005driving,
  title={Driving safety},
  author={Lee, John D},
  journal={Reviews of human factors and ergonomics},
  volume={1},
  number={1},
  pages={172--218},
  year={2005},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{lee2008fifty,
  title={Fifty years of driving safety research},
  author={Lee, John D},
  journal={Human factors},
  volume={50},
  number={3},
  pages={521--528},
  year={2008},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{karrouchi2023driving,
  title={Driving behavior assessment: a practical study and technique for detecting a driver's condition and driving style},
  author={Karrouchi, Mohammed and Nasri, Ismail and Rhiat, Mohammed and Atmane, Ilias and Hirech, Kamal and Messaoudi, Abdelhafid and Melhaoui, Mustapha and Kassmi, Kamal},
  journal={Transportation Engineering},
  volume={14},
  pages={100217},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{aytekin2010increasing,
  title={Increasing driving safety with a multiple vehicle detection and tracking system using ongoing vehicle shadow information},
  author={Aytekin, Burcu and Altu{\u{g}}, Erdin{\c{c}}},
  booktitle={2010 IEEE International Conference on Systems, Man and Cybernetics},
  pages={3650--3656},
  year={2010},
  organization={IEEE}
}

## 02_background

#scene graph
@ARTICLE{9661322,
  author={Chang, Xiaojun and Ren, Pengzhen and Xu, Pengfei and Li, Zhihui and Chen, Xiaojiang and Hauptmann, Alex},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Comprehensive Survey of Scene Graphs: Generation and Application}, 
  year={2023},
  volume={45},
  number={1},
  pages={1-26},
  keywords={Visualization;Task analysis;Feature extraction;Image recognition;Cognition;Training;Systematics;Scene graph;visual feature extraction;prior information;visual relationship recognition},
  doi={10.1109/TPAMI.2021.3137605}}



# NLP
@article{chowdhary2020natural,
  title={Natural language processing},
  author={Chowdhary, KR1442 and Chowdhary, KR},
  journal={Fundamentals of artificial intelligence},
  pages={603--649},
  year={2020},
  publisher={Springer}
}
# attention
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

# graph for video generation
@inproceedings{tang2020unbiased,
  title={Unbiased scene graph generation from biased training},
  author={Tang, Kaihua and Niu, Yulei and Huang, Jianqiang and Shi, Jiaxin and Zhang, Hanwang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3716--3725},
  year={2020}
}

@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019}
}
@inproceedings{xu2017scene,
  title={Scene graph generation by iterative message passing},
  author={Xu, Danfei and Zhu, Yuke and Choy, Christopher B and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5410--5419},
  year={2017}
}

@inproceedings{wang2018videos,
  title={Videos as space-time region graphs},
  author={Wang, Xiaolong and Gupta, Abhinav},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={399--417},
  year={2018}
}

@inproceedings{zellers2018neural,
  title={Neural motifs: Scene graph parsing with global context},
  author={Zellers, Rowan and Yatskar, Mark and Thomson, Sam and Choi, Yejin},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5831--5840},
  year={2018}
}

#GPT
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

#LLM model
@article{alaparthi2020bidirectional,
  title={Bidirectional Encoder Representations from Transformers (BERT): A sentiment analysis odyssey},
  author={Alaparthi, Shivaji and Mishra, Manit},
  journal={arXiv preprint arXiv:2007.01127},
  year={2020}
}

@misc{lewis2019bartdenoisingsequencetosequencepretraining,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.13461}, 
}



# GNN
@article{zhou2020graph,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{wu2020graph,
  title={Graph convolutional networks with markov random field reasoning for social spammer detection},
  author={Wu, Yongji and Lian, Defu and Xu, Yiheng and Wu, Le and Chen, Enhong},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={01},
  pages={1054--1061},
  year={2020}
}

@inproceedings{sanchez2018graph,
  title={Graph networks as learnable physics engines for inference and control},
  author={Sanchez-Gonzalez, Alvaro and Heess, Nicolas and Springenberg, Jost Tobias and Merel, Josh and Riedmiller, Martin and Hadsell, Raia and Battaglia, Peter},
  booktitle={International conference on machine learning},
  pages={4470--4479},
  year={2018},
  organization={PMLR}
}
@article{khalil2017learning,
  title={Learning combinatorial optimization algorithms over graphs},
  author={Khalil, Elias and Dai, Hanjun and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{NIPS2017_f5077839,
 author = {Fout, Alex and Byrd, Jonathon and Shariat, Basir and Ben-Hur, Asa},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Protein Interface Prediction using Graph Convolutional Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f507783927f2ec2737ba40afbd17efb5-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{10.48550/arxiv.2104.13478, 
year = {2021}, 
title = {{Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges}}, 
author = {Bronstein, Michael M and Bruna, Joan and Cohen, Taco and Veličković, Petar}, 
journal = {arXiv}, 
doi = {10.48550/arxiv.2104.13478}, 
eprint = {2104.13478}, 
abstract = {{The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.}}, 
keywords = {}
}

@article{10.1109/tnn.2008.2005605, 
year = {2009}, 
title = {{The Graph Neural Network Model}}, 
author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele}, 
journal = {IEEE Transactions on Neural Networks}, 
issn = {1045-9227}, 
doi = {10.1109/tnn.2008.2005605}, 
pmid = {19068426}, 
abstract = {{Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function \$\textbackslashtau (\{\textbackslashmmb G\},n)\textbackslashin I\textbackslash!\textbackslash!R \textasciicircum\{m\}\$ that maps a graph \$\{\textbackslashmmb G\}\$ and one of its nodes \$n\$ into an \$m\$-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.}}, 
pages = {61--80}, 
number = {1}, 
volume = {20}, 
keywords = {}
}

# GNN with different approaches
@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}

@article{defferrard2016convolutional,
  title={Convolutional neural networks on graphs with fast localized spectral filtering},
  author={Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{wu2019simplifying,
  title={Simplifying graph convolutional networks},
  author={Wu, Felix and Souza, Amauri and Zhang, Tianyi and Fifty, Christopher and Yu, Tao and Weinberger, Kilian},
  booktitle={International conference on machine learning},
  pages={6861--6871},
  year={2019},
  organization={PMLR}
}

@article{velickovic2017graph,
  title={Graph attention networks},
  author={Velickovic, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua and others},
  journal={stat},
  volume={1050},
  number={20},
  pages={10--48550},
  year={2017}
}

@inproceedings{monti2017geometric,
  title={Geometric deep learning on graphs and manifolds using mixture model cnns},
  author={Monti, Federico and Boscaini, Davide and Masci, Jonathan and Rodola, Emanuele and Svoboda, Jan and Bronstein, Michael M},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5115--5124},
  year={2017}
}

@article{zhang2018gaan,
  title={Gaan: Gated attention networks for learning on large and spatiotemporal graphs},
  author={Zhang, Jiani and Shi, Xingjian and Xie, Junyuan and Ma, Hao and King, Irwin and Yeung, Dit-Yan},
  journal={arXiv preprint arXiv:1803.07294},
  year={2018}
}

@inproceedings{gilmer2017neural,
  title={Neural message passing for quantum chemistry},
  author={Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
  booktitle={International conference on machine learning},
  pages={1263--1272},
  year={2017},
  organization={PMLR}
}

@article{battaglia2018relational,
  title={Relational inductive biases, deep learning, and graph networks},
  author={Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others},
  journal={arXiv preprint arXiv:1806.01261},
  year={2018}
}

# RNN
@article{schmidt2019recurrent,
  title={Recurrent neural networks (rnns): A gentle introduction and overview},
  author={Schmidt, Robin M},
  journal={arXiv preprint arXiv:1912.05911},
  year={2019}
}

@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}
@incollection{jordan1997serial,
  title={Serial order: A parallel distributed processing approach},
  author={Jordan, Michael I},
  booktitle={Advances in psychology},
  volume={121},
  pages={471--495},
  year={1997},
  publisher={Elsevier}
}


@article{10.1162/089976699300016890, 
year = {1999}, 
title = {{Modeling and Prediction of Human Behavior}}, 
author = {Pentland, Alex and Liu, Andrew}, 
journal = {Neural Computation}, 
issn = {0899-7667}, 
doi = {10.1162/089976699300016890}, 
pmid = {9950731}, 
abstract = {{We propose that many human behaviors can be accurately described as a set of dynamic models (e.g., Kalman filters) sequenced together by a Markov chain. We then use these dynamic Markov models to recognize human behaviors from sensory data and to predict human behaviors over a few seconds time. To test the power of this modeling approach, we report an experiment in which we were able to achieve 95 accuracy at predicting automobile drivers' subsequent actions from their initial preparatory movements.}}, 
pages = {229--242}, 
number = {1}, 
volume = {11}, 
keywords = {}
}

@article{10.1109/ivs.2018.8500717, 
year = {2018}, 
title = {{Prediction of human driver behaviors based on an improved HMM approach}}, 
author = {Deng, Qi and Wang, Jiao and Soffker, Dirk}, 
journal = {2018 IEEE Intelligent Vehicles Symposium (IV)}, 
doi = {10.1109/ivs.2018.8500717}, 
abstract = {{Research and development of predicting driving behaviors play an important role in the development of Advanced Driver Assistance Systems (ADAS) for assisting drivers. In this contribution, an approach is developed based on Hidden Markov Model (HMM) for predicting human driving behaviors. Three different driving maneuvers including left/right lane change and lane keeping are modeled as hidden states for the HMM. Based on observations (training), the HMM approach is able to calculate the most possible driving behaviors using observed sequences. Furthermore, the observed sequences are also used for training of HMM in the modeling process. To improve the prediction performance of the model, a prefilter is proposed to quantize the collected signals into observed sequences with specific features. In this contribution the definition of a suitable prefilter will be discussed and finally optimized. The approach focuses on the definition of optimal prefilters. Here optimality is defined as the optimal segments describing a quantized prefilter mapping the vehicle's environment to quantized states. In combination with related HMM-based results in terms of accuracy, detection, and false alarm rates an optimal parameter set of the prefilter can be determined. Using experimental data from real human driving behaviors (taken from driving simulator) it can be concluded that the optimal definition of the prefilter can increase the detection rate and accuracy, and in the meanwhile decrease the false alarm rate. The effectiveness of driving behaviors prediction has been successfully proved by comparison with other methods in this contribution.}}, 
pages = {2066--2071}, 
volume = {00}, 
keywords = {}
}

@article{10.1109/icra40945.2020.9196918, 
year = {2020}, 
title = {{Human Driver Behavior Prediction based on UrbanFlow*}}, 
author = {Qiao, Zhiqian and Zhao, Jing and Zhu, Jin and Tyree, Zachariah and Mudalige, Priyantha and Schneider, Jeff and Dolan, John M.}, 
journal = {2020 IEEE International Conference on Robotics and Automation (ICRA)}, 
doi = {10.1109/icra40945.2020.9196918}, 
abstract = {{How autonomous vehicles and human drivers share public transportation systems is an important problem, as fully automatic transportation environments are still a long way off. Understanding human drivers’ behavior can be beneficial for autonomous vehicle decision making and planning, especially when the autonomous vehicle is surrounded by human drivers who have various driving behaviors and patterns of interaction with other vehicles. In this paper, we propose an LSTM-based trajectory prediction method for human drivers which can help the autonomous vehicle make better decisions, especially in urban intersection scenarios. Meanwhile, in order to collect human drivers’ driving behavior data in the urban scenario, we describe a system called UrbanFlow which includes the whole procedure from raw bird’s-eye view data collection via drone to the final processed trajectories. The system is mainly intended for urban scenarios but can be extended to be used for any traffic scenarios.}}, 
pages = {10570--10576}, 
volume = {00}, 
keywords = {}
}

@article{rabiner1989tutorial,
  title={A tutorial on hidden Markov models and selected applications in speech recognition},
  author={Rabiner, Lawrence R},
  journal={Proceedings of the IEEE},
  volume={77},
  number={2},
  pages={257--286},
  year={1989},
  publisher={Ieee}
}
@article{baum1972inequality,
  title={An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes},
  author={Baum, Leonard E and others},
  journal={Inequalities},
  volume={3},
  number={1},
  pages={1--8},
  year={1972}
}

## 03_related_work
# dataset
@INPROCEEDINGS{9009583,
  author={Martin, Manuel and Roitberg, Alina and Haurilet, Monica and Horne, Matthias and Reiß, Simon and Voit, Michael and Stiefelhagen, Rainer},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Drive Act: A Multi-Modal Dataset for Fine-Grained Driver Behavior Recognition in Autonomous Vehicles}, 
  year={2019},
  volume={},
  number={},
  pages={2801-2810},
  keywords={Vehicles;Three-dimensional displays;Task analysis;Cameras;Benchmark testing;Manuals;Skeleton},
  doi={10.1109/ICCV.2019.00289}}

@article{palazzi2018predicting,
  title={Predicting the driver's focus of attention: the dr (eye) ve project},
  author={Palazzi, Andrea and Abati, Davide and Solera, Francesco and Cucchiara, Rita and others},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={7},
  pages={1720--1733},
  year={2018},
  publisher={IEEE}
}

@inproceedings{regan2012naturalistic,
  title={Naturalistic driving studies: literature review and planning for the Australian naturalistic driving study},
  author={Regan, MA and Williamson, A and Grzebieta, R and Tao, L},
  booktitle={Proc. Australasian College Of Road Safety Conference, A Safe System: Expanding The Reach, Sydney},
  year={2012}
}

# theoritical foundation for dynamic network

# anormaly detection
@inproceedings{kopuklu2021driver,
  title={Driver anomaly detection: A dataset and contrastive learning approach},
  author={Kopuklu, Okan and Zheng, Jiapeng and Xu, Hang and Rigoll, Gerhard},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={91--100},
  year={2021}
}

# models
@article{poursafaei2022towards,
  title={Towards better evaluation for dynamic link prediction},
  author={Poursafaei, Farimah and Huang, Shenyang and Pelrine, Kellin and Rabbany, Reihaneh},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32928--32941},
  year={2022}
}
    # CAWN
@article{wang2021inductive,
  title={Inductive representation learning in temporal networks via causal anonymous walks},
  author={Wang, Yanbang and Chang, Yen-Yu and Liu, Yunyu and Leskovec, Jure and Li, Pan},
  journal={arXiv preprint arXiv:2101.05974},
  year={2021}
}

    # TGN
@article{rossi2006temporal,
  title={Temporal graph networks for deep learning on dynamic graphs. arXiv 2020},
  author={Rossi, Emanuele and Chamberlain, Ben and Frasca, Fabrizio and Eynard, Davide and Monti, Federico and Bronstein, Michael},
  journal={arXiv preprint arXiv:2006.10637},
  year={2020}
}

    # TGAT
@article{xu2020inductive,
  title={Inductive representation learning on temporal graphs},
  author={Xu, Da and Ruan, Chuanwei and Korpeoglu, Evren and Kumar, Sushant and Achan, Kannan},
  journal={arXiv preprint arXiv:2002.07962},
  year={2020}
}

    # DyRep
@article{trivedi2018representation,
  title={Representation learning over dynamic graphs},
  author={Trivedi, Rakshit and Farajtabar, Mehrdad and Biswal, Prasenjeet and Zha, Hongyuan},
  journal={arXiv preprint arXiv:1803.04051},
  year={2018}
}

    # JODIE
@inproceedings{kumar2019predicting,
  title={Predicting dynamic embedding trajectory in temporal interaction networks},
  author={Kumar, Srijan and Zhang, Xikun and Leskovec, Jure},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1269--1278},
  year={2019}
}


# HMM
@misc{jurafskyspeech,
  title={Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  author={Jurafsky, Daniel and Martin, James H}
}


