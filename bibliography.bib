## 01_Introduction
@article{inagaki2019critique,
  title={A critique of the SAE conditional driving automation definition, and analyses of options for improvement},
  author={Inagaki, Toshiyuki and Sheridan, Thomas B},
  journal={Cognition, technology \& work},
  volume={21},
  pages={569--578},
  year={2019},
  publisher={Springer}
}

@article{lee2005driving,
  title={Driving safety},
  author={Lee, John D},
  journal={Reviews of human factors and ergonomics},
  volume={1},
  number={1},
  pages={172--218},
  year={2005},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{lee2008fifty,
  title={Fifty years of driving safety research},
  author={Lee, John D},
  journal={Human factors},
  volume={50},
  number={3},
  pages={521--528},
  year={2008},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{karrouchi2023driving,
  title={Driving behavior assessment: a practical study and technique for detecting a driver's condition and driving style},
  author={Karrouchi, Mohammed and Nasri, Ismail and Rhiat, Mohammed and Atmane, Ilias and Hirech, Kamal and Messaoudi, Abdelhafid and Melhaoui, Mustapha and Kassmi, Kamal},
  journal={Transportation Engineering},
  volume={14},
  pages={100217},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{aytekin2010increasing,
  title={Increasing driving safety with a multiple vehicle detection and tracking system using ongoing vehicle shadow information},
  author={Aytekin, Burcu and Altu{\u{g}}, Erdin{\c{c}}},
  booktitle={2010 IEEE International Conference on Systems, Man and Cybernetics},
  pages={3650--3656},
  year={2010},
  organization={IEEE}
}

## 02_background

#scene graph
@ARTICLE{9661322,
  author={Chang, Xiaojun and Ren, Pengzhen and Xu, Pengfei and Li, Zhihui and Chen, Xiaojiang and Hauptmann, Alex},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Comprehensive Survey of Scene Graphs: Generation and Application}, 
  year={2023},
  volume={45},
  number={1},
  pages={1-26},
  keywords={Visualization;Task analysis;Feature extraction;Image recognition;Cognition;Training;Systematics;Scene graph;visual feature extraction;prior information;visual relationship recognition},
  doi={10.1109/TPAMI.2021.3137605}}



# NLP
@article{chowdhary2020natural,
  title={Natural language processing},
  author={Chowdhary, KR1442 and Chowdhary, KR},
  journal={Fundamentals of artificial intelligence},
  pages={603--649},
  year={2020},
  publisher={Springer}
}
# attention
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

# graph for video generation
@inproceedings{tang2020unbiased,
  title={Unbiased scene graph generation from biased training},
  author={Tang, Kaihua and Niu, Yulei and Huang, Jianqiang and Shi, Jiaxin and Zhang, Hanwang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3716--3725},
  year={2020}
}

@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019}
}
@inproceedings{xu2017scene,
  title={Scene graph generation by iterative message passing},
  author={Xu, Danfei and Zhu, Yuke and Choy, Christopher B and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5410--5419},
  year={2017}
}

@inproceedings{wang2018videos,
  title={Videos as space-time region graphs},
  author={Wang, Xiaolong and Gupta, Abhinav},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={399--417},
  year={2018}
}

@inproceedings{zellers2018neural,
  title={Neural motifs: Scene graph parsing with global context},
  author={Zellers, Rowan and Yatskar, Mark and Thomson, Sam and Choi, Yejin},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5831--5840},
  year={2018}
}

#GPT
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

#LLM model
@article{alaparthi2020bidirectional,
  title={Bidirectional Encoder Representations from Transformers (BERT): A sentiment analysis odyssey},
  author={Alaparthi, Shivaji and Mishra, Manit},
  journal={arXiv preprint arXiv:2007.01127},
  year={2020}
}

@misc{lewis2019bartdenoisingsequencetosequencepretraining,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.13461}, 
}



# GNN
@article{zhou2020graph,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{wu2020graph,
  title={Graph convolutional networks with markov random field reasoning for social spammer detection},
  author={Wu, Yongji and Lian, Defu and Xu, Yiheng and Wu, Le and Chen, Enhong},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={01},
  pages={1054--1061},
  year={2020}
}

@inproceedings{sanchez2018graph,
  title={Graph networks as learnable physics engines for inference and control},
  author={Sanchez-Gonzalez, Alvaro and Heess, Nicolas and Springenberg, Jost Tobias and Merel, Josh and Riedmiller, Martin and Hadsell, Raia and Battaglia, Peter},
  booktitle={International conference on machine learning},
  pages={4470--4479},
  year={2018},
  organization={PMLR}
}
@article{khalil2017learning,
  title={Learning combinatorial optimization algorithms over graphs},
  author={Khalil, Elias and Dai, Hanjun and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{NIPS2017_f5077839,
 author = {Fout, Alex and Byrd, Jonathon and Shariat, Basir and Ben-Hur, Asa},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Protein Interface Prediction using Graph Convolutional Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f507783927f2ec2737ba40afbd17efb5-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{10.48550/arxiv.2104.13478, 
year = {2021}, 
title = {{Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges}}, 
author = {Bronstein, Michael M and Bruna, Joan and Cohen, Taco and Veličković, Petar}, 
journal = {arXiv}, 
doi = {10.48550/arxiv.2104.13478}, 
eprint = {2104.13478}, 
abstract = {{The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.}}, 
keywords = {}
}

@article{10.1109/tnn.2008.2005605, 
year = {2009}, 
title = {{The Graph Neural Network Model}}, 
author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele}, 
journal = {IEEE Transactions on Neural Networks}, 
issn = {1045-9227}, 
doi = {10.1109/tnn.2008.2005605}, 
pmid = {19068426}, 
abstract = {{Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function \$\textbackslashtau (\{\textbackslashmmb G\},n)\textbackslashin I\textbackslash!\textbackslash!R \textasciicircum\{m\}\$ that maps a graph \$\{\textbackslashmmb G\}\$ and one of its nodes \$n\$ into an \$m\$-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.}}, 
pages = {61--80}, 
number = {1}, 
volume = {20}, 
keywords = {}
}


## 03_related_work
# dataset
@InProceedings{drive_and_act_2019_iccv,
author = {Martin, Manuel and Roitberg, Alina and Haurilet, Monica and Horne, Matthias and Reiß, Simon and Voit, Michael and Stiefelhagen, Rainer},
title = {Drive&Act: A Multi-modal Dataset for Fine-grained Driver Behavior Recognition in Autonomous Vehicles},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2019}
}
# theoritical foundation for dynamic network

# anormaly detection
@inproceedings{kopuklu2021driver,
  title={Driver anomaly detection: A dataset and contrastive learning approach},
  author={Kopuklu, Okan and Zheng, Jiapeng and Xu, Hang and Rigoll, Gerhard},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={91--100},
  year={2021}
}

# models
@article{poursafaei2022towards,
  title={Towards better evaluation for dynamic link prediction},
  author={Poursafaei, Farimah and Huang, Shenyang and Pelrine, Kellin and Rabbany, Reihaneh},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32928--32941},
  year={2022}
}
    # CAWN
@article{wang2021inductive,
  title={Inductive representation learning in temporal networks via causal anonymous walks},
  author={Wang, Yanbang and Chang, Yen-Yu and Liu, Yunyu and Leskovec, Jure and Li, Pan},
  journal={arXiv preprint arXiv:2101.05974},
  year={2021}
}

    # TGN
@article{rossi2006temporal,
  title={Temporal graph networks for deep learning on dynamic graphs. arXiv 2020},
  author={Rossi, Emanuele and Chamberlain, Ben and Frasca, Fabrizio and Eynard, Davide and Monti, Federico and Bronstein, Michael},
  journal={arXiv preprint arXiv:2006.10637},
  year={2020}
}

    # TGAT
@article{xu2020inductive,
  title={Inductive representation learning on temporal graphs},
  author={Xu, Da and Ruan, Chuanwei and Korpeoglu, Evren and Kumar, Sushant and Achan, Kannan},
  journal={arXiv preprint arXiv:2002.07962},
  year={2020}
}

    # DyRep
@article{trivedi2018representation,
  title={Representation learning over dynamic graphs},
  author={Trivedi, Rakshit and Farajtabar, Mehrdad and Biswal, Prasenjeet and Zha, Hongyuan},
  journal={arXiv preprint arXiv:1803.04051},
  year={2018}
}

    # JODIE
@inproceedings{kumar2019predicting,
  title={Predicting dynamic embedding trajectory in temporal interaction networks},
  author={Kumar, Srijan and Zhang, Xikun and Leskovec, Jure},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1269--1278},
  year={2019}
}


